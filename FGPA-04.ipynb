{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1801390",
   "metadata": {},
   "source": [
    "\n",
    "# Theory\n",
    "\n",
    "The problem at hand is that of division:\n",
    "\n",
    "$$C = \\frac{N}{D}$$\n",
    "\n",
    "The first step to solving this is to split the problem in two: **Reciprocal calculation**, followed by **multiplication**. This is what it looks like:\n",
    "\n",
    "$$C = N \\cdot \\left(\\frac{1}{D}\\right)$$\n",
    "\n",
    "## Newton-Raphson Method\n",
    "\n",
    "This is the generic iterative equation according to Newton's method:\n",
    "\n",
    "$$x_{i+1} = x_i - \\frac{f(x_i)}{f'(x_i)}$$\n",
    "\n",
    "The idea is to find a function $f(x)$ for which $x = \\frac{1}{D}$ is zero. One such function is:\n",
    "\n",
    "$$f(x) = \\frac{1}{x} - D$$\n",
    "\n",
    "*(Substitute $x = \\frac{1}{D}$ in the above equation and it should result in zero)*\n",
    "\n",
    "Next, we find $f'(x)$ and substitute it in the NM equation to give us an equation that allows successive improvements.\n",
    "\n",
    "$$x_{i+1} = x_i - \\frac{\\frac{1}{x_i} - D}{-\\frac{1}{x_i^2}} = x_i \\cdot (2 - D \\cdot x_i)$$\n",
    "\n",
    "---\n",
    "\n",
    "## Initial Approximation to the Reciprocal\n",
    "\n",
    "Here is the final equation for calculating $x_0$, provided $D$ has been scaled to be in the range $[0.5, 1]$:\n",
    "\n",
    "$$x_0 = \\frac{48}{17} - \\frac{32}{17}D$$\n",
    "\n",
    "In numerical algorithms like division, the goal is not necessarily the smallest average error, but guaranteeing the worst-case error ($|\\epsilon_0|$) is as small as possible.\n",
    "\n",
    "We wish to calculate an approximation for the function $1/D$ such that the worst-case error is minimal. The right tool for this is the **Chebyshev Equioscillation Theorem**.\n",
    "\n",
    "Chebyshev approximation is used because it provides the **Best Uniform Approximation** (or **Minimax Approximation**). This means that out of all possible polynomials of a given degree, the Chebyshev method yields the one that minimizes the maximum absolute error across the entire target interval.\n",
    "\n",
    "We start by formulating the error function on which equioscillation will be applied. The error function for figuring out the reciprocal $x_0 = 1/D$ using a simple straight line $x_0 = T_0 + T_1 \\cdot D$ (a linear equation) tells us how far off our guess is from the perfect answer. Because we want the total result $D \\cdot x_0$ to be near $1$, we make the error function $f(D)$ measure the difference between that product and $1$. The formula is $f(D) = 1 - D \\cdot x_0$. When we plug in the straight-line guess, the formula becomes:\n",
    "\n",
    "$$f(D) = 1 - T_0 D - T_1 D^2$$\n",
    "\n",
    "The main goal is to pick the numbers $T_0$ and $T_1$ that minimize the absolute value of this error $f(D)$ everywhere in the range. This is exactly what the Chebyshev method does.\n",
    "\n",
    "We need to constrain the values that $D$ can take. Bounding $D$ guarantees that the starting error is small enough for the subsequent iterations to converge quickly and predictably to full fixed-point precision. Without this bound, a much more complex, higher-degree polynomial would be needed, defeating the efficiency goal. We bound $D$ to be $[0.5, 1]$. In code, this scaling can be achieved through simple bit-shifts. As long as we scale the numerator too, the result remains correct.\n",
    "\n",
    "The theorem states that a polynomial is the best uniform approximation to a continuous function over an interval if and only if the error function alternates between its maximum positive and maximum negative values at least $(n+2)$ times, where $n$ is the degree of the polynomial. Since $n = 1$ (linear approximation), we need $1 + 2 = 3$ alternating extrema: at the two endpoints ($D = 1/2$ and $D = 1$) and the local extremum ($D_{min}$) between them.\n",
    "\n",
    "The location of the local extremum is found by setting the derivative to zero:\n",
    "\n",
    "$$f'(D) = -T_0 - 2T_1 D = 0, \\quad \\text{which gives:} \\quad D_{min} = -\\frac{T_0}{2T_1}$$\n",
    "\n",
    "The first condition of the theorem is that the error magnitude is equal at endpoints, i.e., $f(1/2) = f(1)$.\n",
    "\n",
    "$$1 - \\frac{T_0}{2} - \\frac{T_1}{4} = 1 - T_0 - T_1$$\n",
    "\n",
    "This simplifies to:\n",
    "\n",
    "$$T_0 = -\\frac{3}{2}T_1$$\n",
    "\n",
    "The second condition states that the error at endpoints must be the negative of the error at the extremum, i.e., $f(1) = -f(D_{min})$.\n",
    "\n",
    "$$1 - T_0 - T_1 = -\\left(1 - T_0 D_{min} - T_1 D_{min}^2\\right)$$\n",
    "\n",
    "Substituting $D_{min}$ and simplifying:\n",
    "\n",
    "$$1 - T_0 - T_1 = -\\left(1 + \\frac{T_0^2}{4T_1}\\right)$$\n",
    "\n",
    "Substituting the value of $T_0$ from above:\n",
    "\n",
    "$$1 - \\left(-\\frac{3}{2}T_1\\right) - T_1 = -1 - \\frac{\\left(-3/2 \\cdot T_1\\right)^2}{4T_1}$$\n",
    "\n",
    "$$1 + \\frac{1}{2}T_1 = -1 - \\frac{9T_1}{16}$$\n",
    "\n",
    "Solving this linear equation for $T_1$:\n",
    "\n",
    "$$2 = -\\frac{17T_1}{16}$$\n",
    "\n",
    "$$T_1 = -\\frac{32}{17}$$\n",
    "\n",
    "Substituting $T_1$ back into the original equation to find $T_0$:\n",
    "\n",
    "$$T_0 = -\\frac{3}{2} \\cdot \\left(-\\frac{32}{17}\\right)$$\n",
    "\n",
    "$$T_0 = \\frac{48}{17}$$\n",
    "\n",
    "The resulting linear approximation is $x_0 = T_0 + T_1 D$:\n",
    "\n",
    "$$x_0 = \\frac{48}{17} - \\frac{32}{17}D$$\n",
    "\n",
    "This equation gives the optimal initial estimate for the reciprocal that can be refined by iterations of the Newton-Raphson equation.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f8a731d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece4b90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(D):\n",
    "    # Dividing or multiplying  by 2 can easily be done by bit shifting\n",
    "    scale = 1.0\n",
    "    while(D<0.5 or D>1):\n",
    "        if D<0.5:\n",
    "            D = D*2\n",
    "            scale = scale/2 \n",
    "        else:\n",
    "            D = D/2\n",
    "            scale = scale*2\n",
    "    return D, scale\n",
    "\n",
    "def initialGuess(D):\n",
    "    return 48/17 - (32/17)*D\n",
    "\n",
    "def x_n_plus_1(x_n,D):\n",
    "    return x_n*(2 - D*x_n)\n",
    "\n",
    "def newtonRaphson(d):\n",
    "    D,scale = normalize(d)\n",
    "    x_n = initialGuess(D)\n",
    "    x_n_1 = x_n_plus_1(x_n,D)\n",
    "    while(abs(x_n_1-x_n)>1e-6):\n",
    "        x_n = x_n_1\n",
    "        x_n_1 = x_n_plus_1(x_n,D)\n",
    "    return x_n_1/scale\n",
    "def round_to_int(num):\n",
    "    int_num = int(num) \n",
    "    frac = abs(num - int_num) \n",
    "    if frac >= 0.5:\n",
    "        return int_num + 1 if num >= 0 else int_num - 1\n",
    "    return int_num\n",
    "\n",
    "def fixedPoint(num, m, n):\n",
    "  scale = 2**n\n",
    "  fixed_int = round_to_int(num*scale)\n",
    "  quantized = fixed_int/scale\n",
    "  return quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3b8092d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = [3/21, 7/31, 9/41, 11/51, 17/81, 31/67, 41/91, 51/101, 67/131, 81/151]\n",
    "numerators = [3, 7, 9, 11, 17, 31, 41, 51, 67, 81]\n",
    "denominators = [21, 31, 41, 51, 81, 67, 91, 101, 131, 151]\n",
    "\n",
    "arr_processed = []\n",
    "\n",
    "for i in range(len(arr)):\n",
    "    fraction =  newtonRaphson(denominators[i])\n",
    "    res = numerators[i]*fraction\n",
    "    arr_processed.append(fixedPoint(res, 8, 8))\n",
    "\n",
    "arr_processed = np.array(arr_processed)\n",
    "np.savetxt(\"arr_processed.txt\", arr_processed, fmt='%.8f')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224e5ab7",
   "metadata": {},
   "source": [
    "## Finally here I fecth verilog result and compare them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b27933a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Result\t\tVerilog Result\t\tDifference\n",
      "0.14453125\t\t0.14453125\t\t0.00000000\n",
      "0.22656250\t\t0.22656250\t\t0.00000000\n",
      "0.21875000\t\t0.21875000\t\t0.00000000\n",
      "0.21484375\t\t0.21484375\t\t0.00000000\n",
      "0.21093750\t\t0.21093750\t\t0.00000000\n",
      "0.46093750\t\t0.46093750\t\t0.00000000\n",
      "0.44921875\t\t0.44921875\t\t0.00000000\n",
      "0.50390625\t\t0.50390625\t\t0.00000000\n",
      "0.51171875\t\t0.51171875\t\t0.00000000\n",
      "0.53515625\t\t0.53515625\t\t0.00000000\n"
     ]
    }
   ],
   "source": [
    "arr_processed_verilog = np.loadtxt(\"arr_processed_verilog.txt\")\n",
    "\n",
    "# difference table\n",
    "print(\"Python Result\\t\\tVerilog Result\\t\\tDifference\")\n",
    "for p, v in zip(arr_processed, arr_processed_verilog):\n",
    "    print(f\"{p:.8f}\\t\\t{v:.8f}\\t\\t{abs(p-v):.8f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
